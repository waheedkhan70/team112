# -*- coding: utf-8 -*-
"""Model112e.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VO1z3U6wnP1oBZzS3B87W8pzhYFEZuh8

# ***Enhanced Model on realistic dataset with more improved scores ***
"""

# !pip install category_encoders

# !pip install -U xgboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder
# from xgboost import XGBClassifier

import pandas as pd
from sklearn.preprocessing import LabelEncoder

data = pd.read_csv("3_Transaction_Sheet.csv", on_bad_lines='skip')

# Convert 'Transaction Date' to datetime and extract parts
data['Transaction Date'] = pd.to_datetime(data['Transaction Date'], errors='coerce')
data['Trans_Day'] = data['Transaction Date'].dt.day
data['Trans_Month'] = data['Transaction Date'].dt.month
data['Trans_Weekday'] = data['Transaction Date'].dt.weekday

# Convert 'Transaction Time' and extract hour
data['Transaction Time'] = pd.to_datetime(data['Transaction Time'], format='%H:%M', errors='coerce')
data['Trans_Hour'] = data['Transaction Time'].dt.hour

# Extract city/state
data[['Sender_City', 'Sender_State']] = data['Sender Location'].str.extract(r'([^,]+),\s*([^,]+)')
data[['Receiver_City', 'Receiver_State']] = data['Receiver Location'].str.extract(r'([^,]+),\s*([^,]+)')

# Drop raw columns
data.drop(columns=['Transaction Date', 'Transaction Time', 'Sender Location', 'Receiver Location'], inplace=True)

# Encode categorical columns
label_cols = ['Payment Mode', 'To/Received By', 'Sender A/C', 'Receiver A/C',
              'Sender_City', 'Sender_State', 'Receiver_City', 'Receiver_State']

le = LabelEncoder()
for col in label_cols:
    data[col] = le.fit_transform(data[col].astype(str))

data.head()

# Count classes
class_counts_0, class_counts_1 = data['isFraud'].value_counts()
print("Class 0 (Non-Fraud):", class_counts_0)
print("Class 1 (Fraud):", class_counts_1)

# Split classes
class_0_df = data[data['isFraud'] == 0]
class_1_df = data[data['isFraud'] == 1]

# Balance the data
df_class_0_under = class_0_df.sample(class_counts_1, random_state=42)
data_balanced = pd.concat([df_class_0_under, class_1_df], axis=0)\
                  .sample(frac=1, random_state=42)\
                  .reset_index(drop=True)

print("‚úÖ Balanced DataFrame shape:", data_balanced.shape)

# # ========================================
# # üîß STEP 2: FEATURE ENGINEERING
# # ========================================


import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
import pickle

class FeatureEngineer:
    def __init__(self):
        self.label_encoders = {}
        self.label_cols = ['Payment Mode', 'To/Received By', 'Sender A/C', 'Receiver A/C',
                           'Sender_City', 'Sender_State', 'Receiver_City', 'Receiver_State']

    def fit(self, df):
        """Fit encoders and generate features on training data"""
        df = self._add_features(df.copy(), fit_encoders=True)
        return df

    def transform(self, df):
        """Generate features on new data using stored encoders"""
        return self._add_features(df.copy(), fit_encoders=False)

    def _add_features(self, df, fit_encoders):
        df.rename(columns={'Amount (INR)': 'amount'}, inplace=True)

        # Amount-based features
        df['is_large_amount'] = (df['amount'] > df['amount'].quantile(0.90)).astype(int)
        df['amount_percentile'] = df['amount'].rank(pct=True)

        # Time behavior
        df['is_night'] = df['Trans_Hour'].apply(lambda x: 1 if x < 6 or x > 22 else 0)
        df['hour_bin'] = pd.cut(df['Trans_Hour'], bins=[-1, 6, 12, 18, 24], labels=[0, 1, 2, 3]).astype(int)
        df['is_weekend'] = df['Trans_Weekday'].apply(lambda x: 1 if x >= 5 else 0)

        # Relationship-based features
        df['same_location'] = (df['Sender_City'] == df['Receiver_City']).astype(int)
        df['same_account'] = (df['Sender A/C'] == df['Receiver A/C']).astype(int)

        # Label encoding
        for col in self.label_cols:
            df[col] = df[col].astype(str)
            if fit_encoders:
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col])
                self.label_encoders[col] = le
            else:
                le = self.label_encoders.get(col)
                if le:
                    known_classes = list(le.classes_)
                    df[col] = df[col].apply(lambda x: le.transform([x])[0] if x in known_classes else -1)
                else:
                    df[col] = df[col]

        # Cleanup
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.fillna(0, inplace=True)

        return df

    def save(self, path="feature_engineer.pkl"):
        with open(path, 'wb') as f:
            pickle.dump(self, f)

    @staticmethod
    def load(path="feature_engineer.pkl"):
        with open(path, 'rb') as f:
            return pickle.load(f)

# Create the engineer and apply it to your already preprocessed data
engineer = FeatureEngineer()
data_fe = engineer.fit(data)   # `data` should be your preprocessed DataFrame

# X = Features, y = Target
X = data_fe.drop(columns=['isFraud'])  # data_fe is your final feature-engineered DataFrame
y = data_fe['isFraud']

# Optional: ensure numeric dtype (in case any str slipped through)
X = X.apply(pd.to_numeric, errors='coerce')

# Fill missing values created due to conversion
X.fillna(0, inplace=True)

# Confirm everything is numeric
assert X.dtypes.apply(lambda x: np.issubdtype(x, np.number)).all(), "‚ùå Non-numeric data present!"

# Train-test split
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)

scale_weight = (y_train == 0).sum() / (y_train == 1).sum()

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

# y_scores = best_model.predict_proba(X_test)[:, 1]

# # Set a lower threshold (e.g., 0.3 instead of default 0.5)
# y_pred_custom = (y_scores > 0.3).astype(int)

"""***We will use logistic regression in our model *** *italicized text* ***bold text***"""

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Create pipeline
pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("log_reg", LogisticRegression(
        class_weight='balanced',
        max_iter=1000,
        random_state=42,
        solver='lbfgs'  # or try 'saga' for large datasets
    ))
])

# Fit model
pipeline.fit(X_train, y_train)

# Predict
y_pred = pipeline.predict(X_test)

# Evaluate
from sklearn.metrics import confusion_matrix, classification_report
print("üìä Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

print("\nüìã Classification Report:")
print(classification_report(y_test, y_pred, digits=4))

# precision, recall, thresholds = precision_recall_curve(y_test, y_scores)
# ap = average_precision_score(y_test, y_scores)

# plt.figure(figsize=(7, 5))
# plt.plot(recall, precision, label=f"Avg Precision = {ap:.2f}")
# plt.xlabel("Recall")
# plt.ylabel("Precision")
# plt.title("Precision-Recall Curve")
# plt.legend()
# plt.grid()
# plt.show()

import pickle

with open("logistic_fraud_detector.pkl", "wb") as f:
    pickle.dump(pipeline, f)

def get_transaction_input():
    print("üîê Enter Transaction Details:")

    tx = {
        'Amount (INR)': float(input("üí∞ Amount (INR): ")),
        'Transaction Date': input("üìÖ Transaction Date (YYYY-MM-DD): "),
        'Transaction Time': input("üïí Transaction Time (HH:MM): "),
        'Sender Location': input("üìç Sender Location (City, State): "),
        'Receiver Location': input("üìç Receiver Location (City, State): "),
        'Payment Mode': input("üí≥ Payment Mode (e.g., UPI, Card): "),
        'To/Received By': input("üè¶ To/Received By (Name/Entity): "),
        'Sender A/C': input("üî¢ Sender Account (Any ID): "),
        'Receiver A/C': input("üî¢ Receiver Account (Any ID): ")
    }
    return tx

def process_input(tx):
    df = pd.DataFrame([tx])

    # Try parsing with different formats, coerce errors to NaT
    try:
        df['Transaction Date'] = pd.to_datetime(df['Transaction Date'], errors='coerce')
    except ValueError:
        df['Transaction Date'] = pd.NaT # or handle error appropriately

    try:
        df['Transaction Time'] = pd.to_datetime(df['Transaction Time'], format='%H:%M', errors='coerce')
    except ValueError:
         df['Transaction Time'] = pd.NaT # or handle error appropriately

    df['Trans_Day'] = df['Transaction Date'].dt.day
    df['Trans_Month'] = df['Transaction Date'].dt.month
    df['Trans_Weekday'] = df['Transaction Date'].dt.weekday
    df['Trans_Hour'] = df['Transaction Time'].dt.hour

    df[['Sender_City', 'Sender_State']] = df['Sender Location'].str.extract(r'([^,]+),\s*([^,]+)')
    df[['Receiver_City', 'Receiver_State']] = df['Receiver Location'].str.extract(r'([^,]+),\s*([^,]+)')

    df['is_large_amount'] = (df['Amount (INR)'] > 50000).astype(int)
    df['amount_percentile'] = 0.91
    df['is_night'] = df['Trans_Hour'].apply(lambda x: 1 if x < 6 or x > 22 else 0)

    # Handle potential NaN in Trans_Hour before pd.cut
    if df['Trans_Hour'].isnull().any():
        df['hour_bin'] = -1 # Assign a default value for invalid hours, e.g., -1
    else:
        df['hour_bin'] = pd.cut(df['Trans_Hour'], bins=[-1, 6, 12, 18, 24], labels=[0,1,2,3]).astype(int)

    df['is_weekend'] = df['Trans_Weekday'].apply(lambda x: 1 if x >= 5 else 0)
    df['same_location'] = (df['Sender Location'] == df['Receiver Location']).astype(int)
    df['same_account'] = (df['Sender A/C'] == df['Receiver A/C']).astype(int)

    label_cols = ['Payment Mode', 'To/Received By', 'Sender A/C', 'Receiver A/C',
                  'Sender_City', 'Sender_State', 'Receiver_City', 'Receiver_State']
    for col in label_cols:
        df[col] = df[col].astype('category').cat.codes

    df.rename(columns={'Amount (INR)': 'amount'}, inplace=True)
    df.drop(columns=['Transaction Date', 'Transaction Time', 'Sender Location', 'Receiver Location'], inplace=True)
    df.fillna(0, inplace=True)

    return df

# tx_data = get_transaction_input()
# processed = process_input(tx_data)
#
# # Load the saved model
# with open("logistic_fraud_detector.pkl", "rb") as f:
#     model = pickle.load(f)
#
# prediction = model.predict(processed)[0]
# prob = model.predict_proba(processed)[0][1]
#
# print("\nüîç Prediction:", "üö® FRAUD DETECTED" if prediction == 1 else "‚úÖ Legit Transaction")
# print(f"‚ö†Ô∏è Probability of Fraud: {prob:.2f}")